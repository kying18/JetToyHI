{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment on Bandits",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kying18/JetToyHI/blob/master/PS1_Bandits.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJRLreWTa4T6"
      },
      "source": [
        "# Spring 2021 6.884 Computational Sensorimotor Learning Assignment 1\n",
        "In this assignment, we will learn about multi-armed and contextual bandits.  You will need to <font color='blue'>answer the bolded questions</font> and <font color='blue'>fill in the missing code snippets (marked by **TODO**)</font>.\n",
        "\n",
        "There are 255 total points in this assignment, scaled to be worth 6.25% of your final grade."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npS0XcTswcD3"
      },
      "source": [
        "### Setup\n",
        "\n",
        "Ignore the following skeleton code (imports, plotting)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7NqIj2Ok7YT"
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "import os\n",
        "import gym\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "from copy import deepcopy\n",
        "from tqdm.notebook import tqdm\n",
        "from dataclasses import dataclass\n",
        "from typing import Any\n",
        "mpl.rcParams['figure.dpi']= 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhWjpbrMlEHS"
      },
      "source": [
        "# some util functions\n",
        "def plot(logs, x_key, y_key, legend_key, **kwargs):\n",
        "    nums = len(logs[legend_key].unique())\n",
        "    palette = sns.color_palette(\"hls\", nums)\n",
        "    if 'palette' not in kwargs:\n",
        "        kwargs['palette'] = palette\n",
        "    ax = sns.lineplot(x=x_key, y=y_key, data=logs, hue=legend_key, **kwargs)\n",
        "    return ax\n",
        "\n",
        "def set_random_seed(seed):\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "\n",
        "# set random seed\n",
        "seed = 0\n",
        "set_random_seed(seed=seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnoCSsdTmEaH"
      },
      "source": [
        "# Multi-armed bandits\n",
        "\n",
        "Let us define a multi-armed bandit scenario with 10 arms.  There are two slightly different formulations that are useful: \n",
        "\n",
        "- Stochastic Case: Each arm has a reward of 1, with probability $p \\in [0, 1]$.\n",
        "- Deterministic Case:  Each arm has a reward $r \\in [0,1]$, but the same reward is obtained for every pull. \n",
        "\n",
        "\n",
        "In this assignment, we will work through the stochastic case.  The same insights would apply to the deterministic scenario with variable rewards or even to stochastic setups with variable rewards. \n",
        "\n",
        "To define our bandit, we arbitrarily select probabilities $p$ for each arm and save them as `probs`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECiVDc_ElLPa"
      },
      "source": [
        "numArms = 10\n",
        "probs = [np.random.random() for i in range(numArms)]\n",
        "print(probs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-pVugphxnhP"
      },
      "source": [
        "We then define an environment to evaluate different agent strategies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1S47Fz2em1DV"
      },
      "source": [
        "#To simulate a realistic Bandit scenario, we will make use of the BanditEnv.\n",
        "@dataclass\n",
        "class BanditEnv:\n",
        "    probs: np.ndarray # probabilities of giving positive reward for each arm\n",
        "\n",
        "    def step(self, action):\n",
        "        # Pull arm and get stochastic reward (1 for success, 0 for failure)\n",
        "        return 1 if (np.random.random()  < self.probs[action]) else 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Th8QN6oswC3B"
      },
      "source": [
        "#Code for running the bandit environment. \n",
        "@dataclass\n",
        "class BanditEngine:\n",
        "    probs: np.ndarray\n",
        "    max_steps: int\n",
        "    agent: Any\n",
        "\n",
        "    def __post_init__(self):\n",
        "        self.env = BanditEnv(probs=self.probs)\n",
        "    \n",
        "    def run(self, n_runs=1):\n",
        "        log = []\n",
        "        for i in tqdm(range(n_runs), desc='Runs'):\n",
        "            run_rewards = []\n",
        "            run_actions = []\n",
        "            self.agent.reset()\n",
        "            for t in range(self.max_steps):\n",
        "                action = self.agent.get_action()\n",
        "                reward = self.env.step(action)\n",
        "                self.agent.update_Q(action, reward)\n",
        "                run_actions.append(action)\n",
        "                run_rewards.append(reward)\n",
        "            data = {'reward': run_rewards, \n",
        "                    'action': run_actions, \n",
        "                    'step': np.arange(len(run_rewards))}\n",
        "            if hasattr(self.agent, 'epsilon'):\n",
        "                data['epsilon'] = self.agent.epsilon\n",
        "            run_log = pd.DataFrame(data)\n",
        "            log.append(run_log)\n",
        "        return log"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QH8Z6wKwIcO"
      },
      "source": [
        "#Code for aggregrating results of running an agent in the bandit environment. \n",
        "def bandit_sweep(agents, probs, labels, n_runs=2000, max_steps=500):\n",
        "    logs = dict()\n",
        "    pbar = tqdm(agents)\n",
        "    for idx, agent in enumerate(pbar):\n",
        "        pbar.set_description(f'Alg:{labels[idx]}')\n",
        "        engine = BanditEngine(probs=probs, max_steps=max_steps, agent=agent)\n",
        "        ep_log = engine.run(n_runs)\n",
        "        ep_log = pd.concat(ep_log, ignore_index=True)\n",
        "        ep_log['Alg'] = labels[idx]\n",
        "        logs[f'{labels[idx]}'] = ep_log\n",
        "    logs = pd.concat(logs, ignore_index=True)\n",
        "    return logs\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xj5SyR93v4AR"
      },
      "source": [
        "Credits: The code for Multi-Arm Bandits is inspired from\n",
        "\n",
        "* https://github.com/ShangtongZhang/reinforcement-learning-an-introduction/blob/master/chapter02/ten_armed_testbed.py\n",
        "\n",
        "* https://github.com/lilianweng/multi-armed-bandit/blob/master/solvers.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-RZwbMCbcjR"
      },
      "source": [
        "## Oracle Agent\n",
        "\n",
        "The best agent we could possibly build is one that has access to all the necessary information to make an optimal decision, even if that information would not be available in a real world problem.  We call this an \"oracle agent.\"  \n",
        "\n",
        "Imagine you were to build an Oracle agent for the stochastic multi-armed bandits problem defined by `probs`.  What reward would you get from this agent in expectation?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4XPPVotba9q"
      },
      "source": [
        "#### TODO: find the maximum return with priviledged infromation about the reward distribution [5pts] ####\n",
        "\n",
        "##################################\n",
        "print (f'Max possible reward {oracleReward}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ogXvpFvtgVV"
      },
      "source": [
        "## Random Agent\n",
        "\n",
        "That's pretty high reward!  However, let's say that we don't have access to `probs`, and that the only information we can learn about the environment is through interaction.  This is more akin to a real world bandits problem.\n",
        "\n",
        "One baseline agent we should construct is one that chooses a random action at every timestep.  Fill in the `TODO` in the below agent code to implement this behavior."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sk8d5opRwBHn"
      },
      "source": [
        "#As a baseline, lets first construct a baseline agent that chooses a random action at every timestep. \n",
        "#We will measure how much better we can do. \n",
        "@dataclass\n",
        "class RandomAgent:\n",
        "    num_actions: int\n",
        "\n",
        "    def __post_init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.t = 0\n",
        "        self.action_counts = np.zeros(self.num_actions, dtype=np.int) # action counts n(a)\n",
        "        self.Q = np.zeros(self.num_actions, dtype=np.float) # action value Q(a)\n",
        "\n",
        "    def update_Q(self, action, reward):\n",
        "        pass\n",
        "\n",
        "    def get_action(self):\n",
        "        self.t += 1\n",
        "        #### TODO: get a random action index [5pts]####\n",
        "        selected_action = 0 # placeholder\n",
        "        ##################################\n",
        "\n",
        "        return selected_action"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xg98tgFxxekp"
      },
      "source": [
        "#Create the random agent.\n",
        "agent = RandomAgent(num_actions=len(probs))\n",
        "'''\n",
        "In order to measure average behavior of the agent, we are going to run the agent\n",
        "multiple times and compute the mean reward. The number of runs will be denoted \n",
        "by the variable `n_runs`. The default value is set to 1000, but feel free to reduce it\n",
        "it if its taking too much time. \n",
        "'''\n",
        "n_runs = 1000\n",
        "logs = bandit_sweep([agent], probs, ['Random'], n_runs=n_runs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncw1Xc6oxy9w"
      },
      "source": [
        "plot(logs, x_key='step', y_key='reward', legend_key='Alg', estimator='mean', ci=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qecj5KXmOTJq"
      },
      "source": [
        "#### TODO: plot the reward curve of a random agent, and the average reward of all arms [5pts]####\n",
        "\n",
        "###############################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBXkVnvVcINh"
      },
      "source": [
        "**Analyzing the Results:** \n",
        "- On the x-axis is the number of steps taken by the agent. \n",
        "- On the y-axis is the average reward after $i$ steps.\n",
        "\n",
        "The reward obtained by the random agent is far less that the oracle agent. Regret is defined as the difference between the the reward collected by oracle and the agent under consideration. In the above example, regret is about 0.35. \n",
        "\n",
        "**Note:** that if you use a different random seed to run experiments, you might get a slighly different value of regret. Treat this as a ball park figure. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvYYu1S705iQ"
      },
      "source": [
        "## Explore First Agent\n",
        "\n",
        "In the class we discussed an algorithm to solve bandits where, \n",
        "- For the first N (defined as `max_explore` in the code) steps the agent takes random actions. \n",
        "- The agent identifies the best arm based on these N steps and then only chooses the best arm. \n",
        "\n",
        "We will now implement this agent below.  Fill in the missing code in `update_Q` and `get_action`.  We will store the average reward for each action in the variable `self.Q`, and the count of how many times we've taken each action in `self.action_counts`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTOhqhA4CQL6"
      },
      "source": [
        "#Lets now construct the explore first agent  \n",
        "@dataclass\n",
        "class ExploreFirstAgent:\n",
        "    num_actions: int\n",
        "    max_explore: int\n",
        "\n",
        "    def __post_init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.t = 0\n",
        "        self.action_counts = np.zeros(self.num_actions, dtype=np.int) # action counts n(a)\n",
        "        self.Q = np.zeros(self.num_actions, dtype=np.float) # action value Q(a)\n",
        "\n",
        "    def update_Q(self, action, reward):\n",
        "        # Update Q action-value given (action, reward)\n",
        "        # HINT: Keep track of how good each arm is\n",
        "        #### TODO: update Q value [5pts] ####\n",
        "\n",
        "        ##################################\n",
        "\n",
        "    def get_action(self):\n",
        "        self.t += 1\n",
        "        #### TODO: get action [5pts] ####\n",
        "        selected_action = 0 # placeholder\n",
        "        ##################################\n",
        "\n",
        "        return selected_action        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bai_aCyb0uqD"
      },
      "source": [
        "Great! Now we'll instantiate the engine, and run it with $N=5$ (five steps of exploration, followed by entirely greedy policy)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HziwYmDZK8J5"
      },
      "source": [
        "max_explore = 5\n",
        "agent = ExploreFirstAgent(num_actions=len(probs), max_explore=max_explore)\n",
        "logs = bandit_sweep([agent], probs, ['Explore First 5'], n_runs=1000, max_steps=500)\n",
        "plot(logs, x_key='step', y_key='reward', legend_key='Alg', estimator='mean', ci=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsxhwkOHZEp1"
      },
      "source": [
        "### Explore First v.s. Random Agent\n",
        "\n",
        "The results clearly show that the explore first agent performs better than the random agent. However, it still performs much worse than the oracle. How can we improve our performance? \n",
        "\n",
        "If there are 10 possible actions but the agent only explores for 5 steps, then it is likely it won't find the best arm.  Thus, the policy will be suboptimal. Let's see what happens when we allow the agent to explore for more steps. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vf7iB1VCQkV1"
      },
      "source": [
        "'''\n",
        "What happens if we allow the agent to explore for only 5, 10, 50, 100, 200 steps respectively?\n",
        "'''\n",
        "max_explore_steps = [5, 10, 50, 100, 200]\n",
        "n_runs = 1000\n",
        "#### TODO: run ExploreFirstAgent with different max_explore steps, and plot the reward curves [10pts]####\n",
        "\n",
        "#########################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gv1fjJ6B2dlc"
      },
      "source": [
        "**Analyzing the Results** \n",
        "\n",
        "- Notice that for all agents there is a jump in performance. This corresponds to the time point when they switch from explore only to exploit mode. \n",
        "\n",
        "- The agents that explore for 5, 10 steps are unable to accurately identify the best arm everytime. Their scores are lower than that of agents exploring for 50 or 100 steps. These agents find the optimal arm. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x87xNSz_t21T"
      },
      "source": [
        "## UCB Agent\n",
        "\n",
        "Rather than having a fixed delineation between exploration and exploitation, an agent should be able to figure out when to explore and when to exploit.  This leads us to the UCB agent that we discussed in class. \n",
        "\n",
        "Implement the `update_Q` and `get_action` methods for a UCB agent using the course notes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCl2YW3C3ZLr"
      },
      "source": [
        "**Moving to More Realistic Scenarios**\n",
        "\n",
        "**Question (5pts)**:\n",
        "It's unclear how long the agent should explore before switching to exploit mode.  Can you come up with a strategy to choose a good value of `max_explore`? Can we use such a strategy to deploy a product?\n",
        "\n",
        "**Answer**: \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0FPg7pYQm_I"
      },
      "source": [
        "#### UCB Agent #### \n",
        "@dataclass\n",
        "class UCBAgent:\n",
        "    num_actions: int\n",
        "\n",
        "    def __post_init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.t = 0\n",
        "        self.action_counts = np.zeros(self.num_actions, dtype=np.int) # action counts n(a)\n",
        "        self.Q = np.zeros(self.num_actions, dtype=np.float) # action value Q(a)\n",
        "\n",
        "    def update_Q(self, action, reward):\n",
        "        # Update Q action-value given (action, reward)\n",
        "        #### TODO: Calculate the Q-value [5pts] ####\n",
        "\n",
        "        ###############################\n",
        "        \n",
        "    def get_action(self):\n",
        "        self.t += 1\n",
        "        ## HINT: To avoid a division by zero, you can add a small delta>0 to the denominator\n",
        "        #### TODO: Calculate the exploration bonus [5pts] ####\n",
        "        exploration_bonus = 0 # placeholder\n",
        "        ###########################################\n",
        "        Q_explore = self.Q + exploration_bonus\n",
        "        return np.random.choice(np.where(Q_explore == Q_explore.max())[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrHjDcHrRKt5"
      },
      "source": [
        "#Define the UCB Agent\n",
        "agentUCB = UCBAgent(num_actions=len(probs))\n",
        "#Compute Performance\n",
        "logs = bandit_sweep([agentUCB], probs, ['UCB'], n_runs=1000, max_steps=500)\n",
        "#Plot Performance\n",
        "plot(logs, x_key='step', y_key='reward', legend_key='Alg', estimator='mean', ci=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUe1pKzC58Bt"
      },
      "source": [
        "### UCB v/s Explore-First\n",
        "\n",
        "Now let's compare the reward curves of the `UCB` agent and `Explore First` agent with `max_explore=5`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FvVQJIh4e-Z"
      },
      "source": [
        "**Analyzing the Results**\n",
        "\n",
        "**Question [5pts]**: Why does the UCB algorithm learn slowly (even after 500 steps, the agent still does not reach the maximum reward)?\n",
        "\n",
        "**Answer**:\n",
        "\n",
        "\n",
        "\n",
        "If you are willing to afford the risk of missing out on the optimal policy, one could terminate the exploration bonus in UCB after some time. This hybrid between UCB and Explore-First could work better for your use case. However, remember that UCB requires no tuning because it assumes no extra knowledge about the system.  But, this comes at a cost -- if you have extra knowledge, you can achieve better performance than UCB! How to incoporate this knowledge depends on the nature of available information. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBZcVkX7RxVx"
      },
      "source": [
        "#Now we will compare the UCB agent against the ExploreFirst Agent that only explores for 5 steps.\n",
        "#### TODO: run both algorithms and plot the reward curves (max_explore=5) [10pts] ####\n",
        "#### use legends ['UCB', 'Explore First 5'] respectively \n",
        "#### run each algorithm 1000 times (n_runs=1000), and max_steps=1000\n",
        "\n",
        "##################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvdCHeD_6QDj"
      },
      "source": [
        "**Result Analysis:** UCB outperforms the greedy Explore First agent that only explores for 5 steps. \n",
        "\n",
        "What happens if we allow the agent to explore for more steps?  Run the Explore First agent for 20 steps, and compare the reward to the UCB agent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcMlCj38T0eV"
      },
      "source": [
        "#Lets compare UCB with an agent that explores for twenty steps.\n",
        "#### TODO: run both algorithms and plot the reward curves (max_explore=20) [10pts] #### \n",
        "#### use legends ['UCB', 'Explore First 20'] respectively\n",
        "#### run each algorithm 1000 times (n_runs=1000), and max_steps=1000\n",
        "\n",
        "##################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJDZTED5T-zV"
      },
      "source": [
        "**Question**: In the lecture we studied that the UCB algorithm is optimal. Why then does Explore First perform better?**\n",
        "\n",
        "**Answer**: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVudBSud49Jb"
      },
      "source": [
        "### Skewed Arms Scenario:\n",
        "\n",
        "In the previous example, the probability of each arm providing a return was sampled uniformly from $[0,1]$. Because there were only 10 arms, and some arms had similar returns, by performing 20 random actions it is possible to find the best arm by chance. However, if the reward distributions are very skewed (e.g., only one arm returns rewards with high probability, say 0.9), or there are more arms, more actions may be necessary. In this case the initial exploration phase may not succeed at finding the best arm. Lets see this in practice below. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBZonK7fVaVQ"
      },
      "source": [
        "skewedProbs = [0.1, 0.2, 0.15, 0.21, 0.3, 0.05, 0.9, 0.13, 0.17, 0.07, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
        "#### TODO: compare the reward curves of UCBAgent and ExploreFirstAgent (max_explore=len(skewedProbs)) [10pts] ####\n",
        "#### sweep with n_runs=1000, max_steps=1000\n",
        "\n",
        "###################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUtsHpvsVZX7"
      },
      "source": [
        "In this case, UCB performs better than *Explore First (20)*. It is because exploring for 20 steps is insufficient for this problem. This problem again illustrates that unless one has access to privileged information about the problem, UCB performs the best! \n",
        "\n",
        "Also notice that UCB's reward is still increasing and it hasn't converged to the optimal action yet. Try varying the maximum number of steps to see when UCB converges to the optimal / oracle policy. \n",
        "\n",
        "In other words, `max_explore` is a hyperparameter. Without \"tuning\" it, the method may perform well on some problem instances and poorly on others. An advantage of UCB is its lack of hyperparameters. Next, we'll consider another hyperparameter, $\\epsilon$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8hoeDpluL1D"
      },
      "source": [
        "## Epsilon-greedy Agent\n",
        "\n",
        "Another popular method of simultaneoulsy exploring/exploiting is $\\epsilon$-greedy exploration. The main idea is to:\n",
        "- Sample the (estimated) best action with probability $1-\\epsilon$\n",
        "- Perform a random action with probability $\\epsilon$ \n",
        "\n",
        "By changing $\\epsilon$, we can control if the agent is conservative or exploratory. We will now implement this agent. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fn_YpUt2YRRV"
      },
      "source": [
        "##EpsilonGreedy Agent\n",
        "@dataclass\n",
        "class EpsilonGreedyAgent:\n",
        "    num_actions: int\n",
        "    epsilon: float = 0.1\n",
        "\n",
        "    def __post_init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.action_counts = np.zeros(self.num_actions, dtype=np.int) # action counts n(a)\n",
        "        self.Q = np.zeros(self.num_actions, dtype=np.float) # action value Q(a)\n",
        "\n",
        "    def update_Q(self, action, reward):\n",
        "        # Update Q action-value given (action, reward)\n",
        "        self.action_counts[action] += 1\n",
        "        self.Q[action] += (1.0 / self.action_counts[action]) * (reward - self.Q[action])\n",
        "\n",
        "    def get_action(self):\n",
        "        # Epsilon-greedy policy\n",
        "        #### TODO: Code for exploration [5pts] ####\n",
        "        selected_action = 0 # placeholder\n",
        "        ##################################\n",
        "\n",
        "        return selected_action"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2iHG2uj_B97"
      },
      "source": [
        "**Analyzing Epsilon-Greedy Agents** \n",
        "\n",
        "Notice that the reward of all agents gradually increases (except for $\\epsilon=0$, which is an extremely greedy agent). Also, notice that reward is maxmium for $\\epsilon=0.1$ but decreases for higher values. \n",
        "\n",
        "**Question [5pts]**: Why is the reward lower for higher-values of $\\epsilon$?\n",
        "\n",
        "**Answer**:  \n",
        "\n",
        "\n",
        "**Question [5pts]**: To overcome the issue above, one can try setting $\\epsilon=0$ after some time or adaptively chaning $\\epsilon$. Can you suggest a strategy for varying $\\epsilon$ with time $T$?\n",
        "\n",
        "**Answer**: \n",
        "\n",
        "**Consider**: Compare $\\epsilon$-greedy with UCB and the tradeoffs. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIVpOkS67o8P"
      },
      "source": [
        "#### TODO: show reward curves of an EpsilonGreedyAgent with epsilon=[0, 0.1, 0.2, 0.4] [10pts]####\n",
        "\n",
        "###########################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67iQcpLGs1Nz"
      },
      "source": [
        "# Contextual bandits\n",
        "\n",
        "In this section, we will deal with contextual bandits problem. In contextual bandits, we use contextual information about the observed subject to make subject-specific decisions.  The algorithm we will implement is called [LinUCB](https://arxiv.org/pdf/1003.0146.pdf).\n",
        "\n",
        "As an example, imagine we have a website with 10 products that we'd like to promote. Whenever a user enters the website, the website promotes one product to the user. If the user clicks the product link, then it's a successful promotion (reward is $1$). Otherwise, it's a failed promotion (reward is $0$). Our goal is to optimize the click through rate (CTR), and thus optimize our $$$. \n",
        "\n",
        "We will use a dataset from [here](http://www.cs.columbia.edu/~jebara/6998/dataset.txt) to explore contextual bandits.  The dataset contains a pre-logged array of shape $[10000, 102]$. Each row represents a data point at time step $t$ where $t\\in[0, 9999]$. The first column represents the index of the arm $a_t$ that's chosen ($10$ arms in total). The second column represents the reward $r_t\\in\\{0,1\\}$ received for taking the selected arm. The last $100$ columns represent the context feature vector. \n",
        "\n",
        "The following code is inspired by [this code repository](https://github.com/kfoofw/bandit_simulations)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GS5mRam_uU0a"
      },
      "source": [
        "# Download the dataset\n",
        "!wget http://www.cs.columbia.edu/~jebara/6998/dataset.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubGU1b1sKBxc"
      },
      "source": [
        "# load in the dataset\n",
        "#### TODO: load in the dataset.txt, and extract the data as a numpy array of shape [10000, 102] ####"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIyg3CmnS9W6"
      },
      "source": [
        "#### Contextual bandit environment ####\n",
        "@dataclass\n",
        "class ContextualBanditEnv:\n",
        "    dataset: Any\n",
        "    t: int = 0\n",
        "\n",
        "    def step(self, action):\n",
        "        # if the action matches the recorded action in the dataset, it will\n",
        "        # return the recorded reward in the dataset. Otherwise, it will return\n",
        "        # a reward of None\n",
        "        if action == self.dataset[self.t, 0]:\n",
        "            reward = self.dataset[self.t, 1]\n",
        "        else:\n",
        "            reward = None\n",
        "        self.t += 1\n",
        "        return reward\n",
        "    \n",
        "    def reset(self):\n",
        "        self.t = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSkHxoXI7NxV"
      },
      "source": [
        "Fill in the missing code below to implement the LinUCB agent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CaIHWbxKSRB"
      },
      "source": [
        "#### LinUCB Agent #### \n",
        "@dataclass\n",
        "class LinUCBAgent:\n",
        "    num_actions: int\n",
        "    alpha: float\n",
        "    feature_dim: int\n",
        "\n",
        "    def __post_init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.As = [np.identity(self.feature_dim) for i in range(self.num_actions)]\n",
        "        self.bs = [np.zeros([self.feature_dim, 1]) for i in range(self.num_actions)]\n",
        "      \n",
        "    def get_ucb(self, action, state):\n",
        "        #### TODO: compute the UCB of the selected action/arm, and the context information [5pts] ####\n",
        "\n",
        "        return p\n",
        "\n",
        "    def update_params(self, action, reward, state):\n",
        "        #### update A matrix and b matrix given the observed reward, ####\n",
        "        #### selected action, and the context feature                ####\n",
        "        if reward is None:\n",
        "            return        \n",
        "        #### TODO: update A and b matrices of the selected arm [5pts] ####\n",
        "\n",
        "        \n",
        "    def get_action(self, state):\n",
        "        #### find the action given the context information ####\n",
        "\n",
        "        #### TODO: get the UCB estimates for all actions [5pts] ####\n",
        "\n",
        "        #### TODO: choose an arm a_t=\\arg\\max_a(p_{t,a}) with ties broken arbitrarily [5pts] ####\n",
        "        selected_action = 0 # placeholder\n",
        "\n",
        "        return selected_action\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-nzuQgFpqSC"
      },
      "source": [
        "#Code for running the contextual bandit environment. \n",
        "@dataclass\n",
        "class CtxBanditEngine:\n",
        "    dataset: Any\n",
        "    agent: Any\n",
        "\n",
        "    def __post_init__(self):\n",
        "        self.env = ContextualBanditEnv(dataset=self.dataset)\n",
        "    \n",
        "    def run(self, n_runs=1):\n",
        "        log = []\n",
        "        for i in tqdm(range(n_runs), desc='Runs'):\n",
        "            # we only record the time steps when the selected arm matches the arm in the pre-logged data\n",
        "            aligned_ctr = []\n",
        "            ret_val = 0\n",
        "            valid_time_steps = 0\n",
        "            self.env.reset()\n",
        "            self.agent.reset()\n",
        "            for t in tqdm(range(self.dataset.shape[0]), desc='Time'):\n",
        "                state=self.dataset[t, 2:]\n",
        "                action = self.agent.get_action(state=state)\n",
        "                reward = self.env.step(action)\n",
        "                self.agent.update_params(action, reward, state=state)\n",
        "                if reward is not None:\n",
        "                    ret_val += reward\n",
        "                    valid_time_steps += 1\n",
        "                    aligned_ctr.append(ret_val / float(valid_time_steps))\n",
        "            data = {'aligned_ctr': aligned_ctr, \n",
        "                    'step': np.arange(len(aligned_ctr))}\n",
        "            if hasattr(self.agent, 'alpha'):\n",
        "                data['alpha'] = self.agent.alpha\n",
        "            run_log = pd.DataFrame(data)\n",
        "            log.append(run_log)\n",
        "        return log"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHuRQByTKStT"
      },
      "source": [
        "#Code for aggregrating results of running an agent in the contextual bandit environment. \n",
        "def ctxbandit_sweep(alphas, dataset, n_runs=2000):\n",
        "    logs = dict()\n",
        "    pbar = tqdm(alphas)\n",
        "    for idx, alpha in enumerate(pbar):\n",
        "        pbar.set_description(f'alpha:{alpha}')\n",
        "        agent = LinUCBAgent(num_actions=10, feature_dim=100, alpha=alpha)\n",
        "        engine = CtxBanditEngine(dataset=dataset, agent=agent)\n",
        "        ep_log = engine.run(n_runs)\n",
        "        ep_log = pd.concat(ep_log, ignore_index=True)\n",
        "        ep_log['alpha'] = alpha\n",
        "        logs[f'{alpha}'] = ep_log\n",
        "    logs = pd.concat(logs, ignore_index=True)\n",
        "    return logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CzDa5NLszFb"
      },
      "source": [
        "#### TODO: run the sweep with alpha = [0, 0.01, 0.1, 0.5] and n_runs=1 [5pts] ####\n",
        "\n",
        "#########################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmGCmMGnYQ17"
      },
      "source": [
        "plot(logs, x_key='step', y_key='aligned_ctr', legend_key='alpha', estimator='mean', ci=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6ZuIGloOkmR"
      },
      "source": [
        "**Question [5pts]**: What does $\\alpha$ affect in LinUCB?\n",
        "\n",
        "**Answer**: \n",
        "\n",
        "**Question [5pts]**: Do the reward curves change with $\\alpha$? If yes, why? If not, why not?\n",
        "\n",
        "**Answer**: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKE5FDwhy8tk"
      },
      "source": [
        "#### TODO: modify the UCB agent in the multi-armed bandits, and compare the\n",
        "#### aligned_ctr curves between bandit, and contextual bandit with alpha as 0, 0.01, 0.5  [10pts] ####\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tz01_1DF-IrL"
      },
      "source": [
        "**Question [20pts]**: Does LinUCB outperform UCB? If yes, explain why. If not, explain why not. \n",
        "\n",
        "**Answer**: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mtr_xFAh-fQc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}